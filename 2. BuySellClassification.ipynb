{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pickle import dump\n",
    "import warnings\n",
    "from keras.optimizers import Adam\n",
    "from keras import models\n",
    "from keras.layers import Dropout, Dense, GRU, LSTM, Flatten\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Przygotowanie danych"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "               Open     High      Low    Close  ZigZag  Return_rate  \\\nDate                                                                  \n2000-01-18  1.01110  1.01550  1.00720  1.01400       1     1.002769   \n2000-01-19  1.01410  1.01650  1.00850  1.01170       1     0.997732   \n2000-01-20  1.01160  1.01880  1.00730  1.01690       1     1.005140   \n2000-01-21  1.01660  1.01930  1.00500  1.00870       1     0.991936   \n2000-01-23  1.00280  1.00450  1.00260  1.00430       1     0.995638   \n...             ...      ...      ...      ...     ...          ...   \n2022-12-07  1.04670  1.05493  1.04427  1.05114       0     1.004223   \n2022-12-08  1.05115  1.05643  1.04892  1.05551       0     1.004157   \n2022-12-09  1.05549  1.05878  1.05032  1.05325       0     0.997859   \n2022-12-11  1.05286  1.05371  1.05176  1.05204       0     0.998851   \n2022-12-12  1.05204  1.05795  1.05054  1.05407       0     1.001930   \n\n                RSI_7     RSI_14      MA_7     MA_14  \nDate                                                  \n2000-01-18  36.106221  41.847381  1.019714  1.024307  \n2000-01-19  32.411299  39.100201  1.016786  1.023271  \n2000-01-20  46.777470  47.492823  1.014700  1.022393  \n2000-01-21  33.627629  38.485863  1.012514  1.020707  \n2000-01-23  28.595350  34.684738  1.011386  1.018836  \n...               ...        ...       ...       ...  \n2022-12-07  64.103095  64.267401  1.050111  1.043287  \n2022-12-08  69.392195  66.614241  1.052003  1.045037  \n2022-12-09  63.726914  64.263552  1.052074  1.045859  \n2022-12-11  60.634794  62.982086  1.051797  1.046666  \n2022-12-12  64.049088  64.269381  1.051760  1.047780  \n\n[7140 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>ZigZag</th>\n      <th>Return_rate</th>\n      <th>RSI_7</th>\n      <th>RSI_14</th>\n      <th>MA_7</th>\n      <th>MA_14</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2000-01-18</th>\n      <td>1.01110</td>\n      <td>1.01550</td>\n      <td>1.00720</td>\n      <td>1.01400</td>\n      <td>1</td>\n      <td>1.002769</td>\n      <td>36.106221</td>\n      <td>41.847381</td>\n      <td>1.019714</td>\n      <td>1.024307</td>\n    </tr>\n    <tr>\n      <th>2000-01-19</th>\n      <td>1.01410</td>\n      <td>1.01650</td>\n      <td>1.00850</td>\n      <td>1.01170</td>\n      <td>1</td>\n      <td>0.997732</td>\n      <td>32.411299</td>\n      <td>39.100201</td>\n      <td>1.016786</td>\n      <td>1.023271</td>\n    </tr>\n    <tr>\n      <th>2000-01-20</th>\n      <td>1.01160</td>\n      <td>1.01880</td>\n      <td>1.00730</td>\n      <td>1.01690</td>\n      <td>1</td>\n      <td>1.005140</td>\n      <td>46.777470</td>\n      <td>47.492823</td>\n      <td>1.014700</td>\n      <td>1.022393</td>\n    </tr>\n    <tr>\n      <th>2000-01-21</th>\n      <td>1.01660</td>\n      <td>1.01930</td>\n      <td>1.00500</td>\n      <td>1.00870</td>\n      <td>1</td>\n      <td>0.991936</td>\n      <td>33.627629</td>\n      <td>38.485863</td>\n      <td>1.012514</td>\n      <td>1.020707</td>\n    </tr>\n    <tr>\n      <th>2000-01-23</th>\n      <td>1.00280</td>\n      <td>1.00450</td>\n      <td>1.00260</td>\n      <td>1.00430</td>\n      <td>1</td>\n      <td>0.995638</td>\n      <td>28.595350</td>\n      <td>34.684738</td>\n      <td>1.011386</td>\n      <td>1.018836</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2022-12-07</th>\n      <td>1.04670</td>\n      <td>1.05493</td>\n      <td>1.04427</td>\n      <td>1.05114</td>\n      <td>0</td>\n      <td>1.004223</td>\n      <td>64.103095</td>\n      <td>64.267401</td>\n      <td>1.050111</td>\n      <td>1.043287</td>\n    </tr>\n    <tr>\n      <th>2022-12-08</th>\n      <td>1.05115</td>\n      <td>1.05643</td>\n      <td>1.04892</td>\n      <td>1.05551</td>\n      <td>0</td>\n      <td>1.004157</td>\n      <td>69.392195</td>\n      <td>66.614241</td>\n      <td>1.052003</td>\n      <td>1.045037</td>\n    </tr>\n    <tr>\n      <th>2022-12-09</th>\n      <td>1.05549</td>\n      <td>1.05878</td>\n      <td>1.05032</td>\n      <td>1.05325</td>\n      <td>0</td>\n      <td>0.997859</td>\n      <td>63.726914</td>\n      <td>64.263552</td>\n      <td>1.052074</td>\n      <td>1.045859</td>\n    </tr>\n    <tr>\n      <th>2022-12-11</th>\n      <td>1.05286</td>\n      <td>1.05371</td>\n      <td>1.05176</td>\n      <td>1.05204</td>\n      <td>0</td>\n      <td>0.998851</td>\n      <td>60.634794</td>\n      <td>62.982086</td>\n      <td>1.051797</td>\n      <td>1.046666</td>\n    </tr>\n    <tr>\n      <th>2022-12-12</th>\n      <td>1.05204</td>\n      <td>1.05795</td>\n      <td>1.05054</td>\n      <td>1.05407</td>\n      <td>0</td>\n      <td>1.001930</td>\n      <td>64.049088</td>\n      <td>64.269381</td>\n      <td>1.051760</td>\n      <td>1.047780</td>\n    </tr>\n  </tbody>\n</table>\n<p>7140 rows Ã— 10 columns</p>\n</div>"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('EURUSD_2000_2023.csv', parse_dates=True)\n",
    "dataset.Date = pd.to_datetime(dataset.Date, dayfirst=True)\n",
    "dataset = dataset.set_index(dataset.Date)\n",
    "dataset = dataset.drop(\"Date\", axis='columns')\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "X_value = pd.DataFrame(dataset.iloc[:, :])\n",
    "X_value = X_value.drop(\"ZigZag\", axis='columns')\n",
    "y_value = pd.DataFrame(dataset.ZigZag)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "               Open     High      Low    Close  Return_rate      RSI_7  \\\nDate                                                                     \n2000-01-18  1.01110  1.01550  1.00720  1.01400     1.002769  36.106221   \n2000-01-19  1.01410  1.01650  1.00850  1.01170     0.997732  32.411299   \n2000-01-20  1.01160  1.01880  1.00730  1.01690     1.005140  46.777470   \n2000-01-21  1.01660  1.01930  1.00500  1.00870     0.991936  33.627629   \n2000-01-23  1.00280  1.00450  1.00260  1.00430     0.995638  28.595350   \n...             ...      ...      ...      ...          ...        ...   \n2022-12-07  1.04670  1.05493  1.04427  1.05114     1.004223  64.103095   \n2022-12-08  1.05115  1.05643  1.04892  1.05551     1.004157  69.392195   \n2022-12-09  1.05549  1.05878  1.05032  1.05325     0.997859  63.726914   \n2022-12-11  1.05286  1.05371  1.05176  1.05204     0.998851  60.634794   \n2022-12-12  1.05204  1.05795  1.05054  1.05407     1.001930  64.049088   \n\n               RSI_14      MA_7     MA_14  \nDate                                       \n2000-01-18  41.847381  1.019714  1.024307  \n2000-01-19  39.100201  1.016786  1.023271  \n2000-01-20  47.492823  1.014700  1.022393  \n2000-01-21  38.485863  1.012514  1.020707  \n2000-01-23  34.684738  1.011386  1.018836  \n...               ...       ...       ...  \n2022-12-07  64.267401  1.050111  1.043287  \n2022-12-08  66.614241  1.052003  1.045037  \n2022-12-09  64.263552  1.052074  1.045859  \n2022-12-11  62.982086  1.051797  1.046666  \n2022-12-12  64.269381  1.051760  1.047780  \n\n[7140 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Return_rate</th>\n      <th>RSI_7</th>\n      <th>RSI_14</th>\n      <th>MA_7</th>\n      <th>MA_14</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2000-01-18</th>\n      <td>1.01110</td>\n      <td>1.01550</td>\n      <td>1.00720</td>\n      <td>1.01400</td>\n      <td>1.002769</td>\n      <td>36.106221</td>\n      <td>41.847381</td>\n      <td>1.019714</td>\n      <td>1.024307</td>\n    </tr>\n    <tr>\n      <th>2000-01-19</th>\n      <td>1.01410</td>\n      <td>1.01650</td>\n      <td>1.00850</td>\n      <td>1.01170</td>\n      <td>0.997732</td>\n      <td>32.411299</td>\n      <td>39.100201</td>\n      <td>1.016786</td>\n      <td>1.023271</td>\n    </tr>\n    <tr>\n      <th>2000-01-20</th>\n      <td>1.01160</td>\n      <td>1.01880</td>\n      <td>1.00730</td>\n      <td>1.01690</td>\n      <td>1.005140</td>\n      <td>46.777470</td>\n      <td>47.492823</td>\n      <td>1.014700</td>\n      <td>1.022393</td>\n    </tr>\n    <tr>\n      <th>2000-01-21</th>\n      <td>1.01660</td>\n      <td>1.01930</td>\n      <td>1.00500</td>\n      <td>1.00870</td>\n      <td>0.991936</td>\n      <td>33.627629</td>\n      <td>38.485863</td>\n      <td>1.012514</td>\n      <td>1.020707</td>\n    </tr>\n    <tr>\n      <th>2000-01-23</th>\n      <td>1.00280</td>\n      <td>1.00450</td>\n      <td>1.00260</td>\n      <td>1.00430</td>\n      <td>0.995638</td>\n      <td>28.595350</td>\n      <td>34.684738</td>\n      <td>1.011386</td>\n      <td>1.018836</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2022-12-07</th>\n      <td>1.04670</td>\n      <td>1.05493</td>\n      <td>1.04427</td>\n      <td>1.05114</td>\n      <td>1.004223</td>\n      <td>64.103095</td>\n      <td>64.267401</td>\n      <td>1.050111</td>\n      <td>1.043287</td>\n    </tr>\n    <tr>\n      <th>2022-12-08</th>\n      <td>1.05115</td>\n      <td>1.05643</td>\n      <td>1.04892</td>\n      <td>1.05551</td>\n      <td>1.004157</td>\n      <td>69.392195</td>\n      <td>66.614241</td>\n      <td>1.052003</td>\n      <td>1.045037</td>\n    </tr>\n    <tr>\n      <th>2022-12-09</th>\n      <td>1.05549</td>\n      <td>1.05878</td>\n      <td>1.05032</td>\n      <td>1.05325</td>\n      <td>0.997859</td>\n      <td>63.726914</td>\n      <td>64.263552</td>\n      <td>1.052074</td>\n      <td>1.045859</td>\n    </tr>\n    <tr>\n      <th>2022-12-11</th>\n      <td>1.05286</td>\n      <td>1.05371</td>\n      <td>1.05176</td>\n      <td>1.05204</td>\n      <td>0.998851</td>\n      <td>60.634794</td>\n      <td>62.982086</td>\n      <td>1.051797</td>\n      <td>1.046666</td>\n    </tr>\n    <tr>\n      <th>2022-12-12</th>\n      <td>1.05204</td>\n      <td>1.05795</td>\n      <td>1.05054</td>\n      <td>1.05407</td>\n      <td>1.001930</td>\n      <td>64.049088</td>\n      <td>64.269381</td>\n      <td>1.051760</td>\n      <td>1.047780</td>\n    </tr>\n  </tbody>\n</table>\n<p>7140 rows Ã— 9 columns</p>\n</div>"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "y_value = tf.keras.utils.to_categorical(y_value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 1.],\n       [0., 1.],\n       [0., 1.],\n       ...,\n       [1., 0.],\n       [1., 0.],\n       [1., 0.]], dtype=float32)"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "# Normalized the data\n",
    "X_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_scaler.fit(X_value)\n",
    "X_scale_dataset = X_scaler.fit_transform(X_value)\n",
    "dump(X_scaler, open('ClassificationData/X_scaler.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "n_steps_in = 10\n",
    "n_features = X_value.shape[1]\n",
    "n_steps_out = 1\n",
    "output_features = 2\n",
    "print(n_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "def get_X_y(X_data, y_data):\n",
    "    X = list()\n",
    "    y = list()\n",
    "\n",
    "    length = len(X_data)\n",
    "    for i in range(0, length-n_steps_in-n_steps_out, 1):\n",
    "        X_value = X_data[i: i + n_steps_in][:, :]\n",
    "        # y_value = y_data[i + n_steps_in: i + (n_steps_in + n_steps_out)][:, :]\n",
    "        y_value = y_data[i + (n_steps_in + 1)]\n",
    "        if len(X_value) == n_steps_in and len(y_value) == output_features:\n",
    "            X.append(X_value)\n",
    "            y.append(y_value)\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "def predict_index(dataset, X_train, n_steps_in, n_steps_out):\n",
    "\n",
    "    train_predict_index = dataset.iloc[n_steps_in : X_train.shape[0] + n_steps_in + n_steps_out - 1, :].index\n",
    "    test_predict_index = dataset.iloc[X_train.shape[0] + n_steps_in:, :].index\n",
    "\n",
    "    return train_predict_index, test_predict_index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "def split_train_test(data):\n",
    "    train_size = round(len(X) * 0.7)\n",
    "    data_train = data[0:train_size]\n",
    "    data_test = data[train_size:]\n",
    "    return data_train, data_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (7129, 10, 9)\n",
      "y shape:  (7129, 2)\n",
      "X_train shape:  (4990, 10, 9)\n",
      "y_train shape:  (4990, 2)\n",
      "X_test shape:  (2139, 10, 9)\n",
      "y_test shape:  (2139, 2)\n",
      "index_train shape: (4990,)\n",
      "index_test shape: (2140,)\n"
     ]
    }
   ],
   "source": [
    "X, y = get_X_y(X_scale_dataset, y_value)\n",
    "X_train, X_test, = split_train_test(X)\n",
    "y_train, y_test, = split_train_test(y)\n",
    "train_predict_index, test_predict_index, = predict_index(dataset, X_train, n_steps_in, n_steps_out)\n",
    "print('X shape: ', X.shape)\n",
    "print('y shape: ', y.shape)\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('y_test shape: ', y_test.shape)\n",
    "print('index_train shape:', train_predict_index.shape)\n",
    "print('index_test shape:', test_predict_index.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "def get_simulation_data(test_index, output_dim):\n",
    "    org_dataset = pd.read_csv('EURUSD_DATASET.csv', parse_dates=True)\n",
    "    org_dataset.Date = pd.to_datetime(org_dataset.Date, dayfirst=True)\n",
    "    org_dataset = org_dataset.set_index(org_dataset.Date)\n",
    "    org_dataset = org_dataset.drop(\"ZigZag\", axis='columns')\n",
    "\n",
    "    sim_indexes = pd.DataFrame()\n",
    "    sim_indexes['Date'] = test_index\n",
    "    sim_indexes['Date'] = pd.to_datetime(sim_indexes['Date'], dayfirst=True)\n",
    "    sim_indexes = sim_indexes.set_index(sim_indexes['Date'])\n",
    "    sim_indexes.drop(\"Date\", axis='columns')\n",
    "\n",
    "    sim_dataset = org_dataset.loc[org_dataset.index.isin(sim_indexes.index)]\n",
    "    cutDays = output_dim - 1\n",
    "    sim_dataset = sim_dataset.iloc[:-cutDays]\n",
    "\n",
    "    sim_dataset.to_csv(\"ClassificationData/Simulation_Dataset.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[-0.52497399, -0.52567427, -0.51896754, ..., -0.21667705,\n         -0.51180565, -0.50579834],\n        [-0.51716961, -0.52308091, -0.51557815, ..., -0.29164398,\n         -0.51958126, -0.50858925],\n        [-0.52367326, -0.51711618, -0.51870682, ..., -0.06262035,\n         -0.525119  , -0.5109567 ],\n        ...,\n        [-0.55072841, -0.55160788, -0.54060748, ..., -0.48406236,\n         -0.54571488, -0.53538192],\n        [-0.55463059, -0.55809129, -0.57215487, ..., -0.70001749,\n         -0.55508354, -0.54408184],\n        [-0.58688866, -0.59387967, -0.6065702 , ..., -0.83150061,\n         -0.57101405, -0.55497599]],\n\n       [[-0.51716961, -0.52308091, -0.51557815, ..., -0.29164398,\n         -0.51958126, -0.50858925],\n        [-0.52367326, -0.51711618, -0.51870682, ..., -0.06262035,\n         -0.525119  , -0.5109567 ],\n        [-0.51066597, -0.5158195 , -0.52470343, ..., -0.30840845,\n         -0.53092226, -0.51549914],\n        ...,\n        [-0.55463059, -0.55809129, -0.57215487, ..., -0.70001749,\n         -0.55508354, -0.54408184],\n        [-0.58688866, -0.59387967, -0.6065702 , ..., -0.83150061,\n         -0.57101405, -0.55497599],\n        [-0.61290323, -0.61670124, -0.5971842 , ..., -0.66180736,\n         -0.58205162, -0.56352193]],\n\n       [[-0.52367326, -0.51711618, -0.51870682, ..., -0.06262035,\n         -0.525119  , -0.5109567 ],\n        [-0.51066597, -0.5158195 , -0.52470343, ..., -0.30840845,\n         -0.53092226, -0.51549914],\n        [-0.54656608, -0.55420124, -0.53096076, ..., -0.41213616,\n         -0.53391872, -0.52054201],\n        ...,\n        [-0.58688866, -0.59387967, -0.6065702 , ..., -0.83150061,\n         -0.57101405, -0.55497599],\n        [-0.61290323, -0.61670124, -0.5971842 , ..., -0.66180736,\n         -0.58205162, -0.56352193],\n        [-0.60691988, -0.61021784, -0.62456003, ..., -0.75828646,\n         -0.59487189, -0.57154818]],\n\n       ...,\n\n       [[-0.45681582, -0.46716805, -0.44254986, ...,  0.32855856,\n         -0.47079216, -0.47710015],\n        [-0.45850676, -0.4371888 , -0.45172728, ...,  0.27348034,\n         -0.47031046, -0.47730225],\n        [-0.46462019, -0.46374481, -0.45467345, ...,  0.22652656,\n         -0.46733676, -0.47736385],\n        ...,\n        [-0.41248699, -0.41174793, -0.41264503, ...,  0.40121431,\n         -0.4429479 , -0.46242   ],\n        [-0.42479188, -0.4277749 , -0.41809412, ...,  0.32620901,\n         -0.43823702, -0.459785  ],\n        [-0.43236212, -0.42341805, -0.42231782, ...,  0.39513582,\n         -0.43109864, -0.45465359]],\n\n       [[-0.45850676, -0.4371888 , -0.45172728, ...,  0.27348034,\n         -0.47031046, -0.47730225],\n        [-0.46462019, -0.46374481, -0.45467345, ...,  0.22652656,\n         -0.46733676, -0.47736385],\n        [-0.46969303, -0.45446058, -0.46220832, ...,  0.38060229,\n         -0.46306587, -0.47537172],\n        ...,\n        [-0.42479188, -0.4277749 , -0.41809412, ...,  0.32620901,\n         -0.43823702, -0.459785  ],\n        [-0.43236212, -0.42341805, -0.42231782, ...,  0.39513582,\n         -0.43109864, -0.45465359],\n        [-0.42078564, -0.41952801, -0.41019424, ...,  0.459178  ,\n         -0.42607673, -0.44993793]],\n\n       [[-0.46462019, -0.46374481, -0.45467345, ...,  0.22652656,\n         -0.46733676, -0.47736385],\n        [-0.46969303, -0.45446058, -0.46220832, ...,  0.38060229,\n         -0.46306587, -0.47537172],\n        [-0.44396462, -0.42608921, -0.43532786, ...,  0.5117797 ,\n         -0.45889359, -0.47209   ],\n        ...,\n        [-0.43236212, -0.42341805, -0.42231782, ...,  0.39513582,\n         -0.43109864, -0.45465359],\n        [-0.42078564, -0.41952801, -0.41019424, ...,  0.459178  ,\n         -0.42607673, -0.44993793],\n        [-0.40949532, -0.41343361, -0.40654413, ...,  0.39503077,\n         -0.42588708, -0.44772445]]])"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 0.],\n       [1., 0.],\n       [1., 0.],\n       ...,\n       [1., 0.],\n       [1., 0.],\n       [1., 0.]], dtype=float32)"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "np.save(\"ClassificationData/X_train.npy\", X_train)\n",
    "np.save(\"ClassificationData/y_train.npy\", y_train)\n",
    "np.save(\"ClassificationData/X_test.npy\", X_test)\n",
    "np.save(\"ClassificationData/y_test.npy\", y_test)\n",
    "np.save('ClassificationData/train_predict_index.npy', train_predict_index)\n",
    "np.save('ClassificationData/test_predict_index.npy', test_predict_index)\n",
    "get_simulation_data(test_predict_index, n_steps_out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Wytrenowanie klasyfikatora"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train = np.load(\"ClassificationData/X_train.npy\", allow_pickle=True)\n",
    "y_train = np.load(\"ClassificationData/y_train.npy\", allow_pickle=True)\n",
    "X_test = np.load(\"ClassificationData/X_test.npy\", allow_pickle=True)\n",
    "y_test = np.load(\"ClassificationData/y_test.npy\", allow_pickle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "# LR = 0.0005\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCH = 100\n",
    "\n",
    "input_dim = X_train.shape[1]     # 7\n",
    "input_feature_size = X_train.shape[2]  # 5\n",
    "output_dim = y_train.shape[1]    # 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(output_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 0.],\n       [1., 0.],\n       [1., 0.],\n       ...,\n       [1., 0.],\n       [1., 0.],\n       [1., 0.]], dtype=float32)"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "def basic_GRU(input_dim, output_dim, input_feature_size):\n",
    "    model = models.Sequential()\n",
    "    model.add(GRU(units=63, return_sequences=True, activation='relu', input_shape=(input_dim, input_feature_size)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(units=250, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=N_EPOCH, batch_size=BATCH_SIZE, verbose=2, shuffle=False)\n",
    "\n",
    "    return model, history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "## 61.92% ###\n",
    "\n",
    "def basic_LSTM(input_dim, output_dim, input_feature_size):\n",
    "    model = models.Sequential()\n",
    "    model.add(LSTM(units=63, return_sequences=True, activation='relu', input_shape=(input_dim, input_feature_size)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(250, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=N_EPOCH, batch_size=BATCH_SIZE, verbose=2, shuffle=False)\n",
    "\n",
    "    return model, history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "# def basic_LSTM(input_dim, output_dim, input_feature_size):\n",
    "#     model = models.Sequential()\n",
    "#     model.add(LSTM(units=50, return_sequences=True, input_shape=(input_dim, input_feature_size)))\n",
    "#     # model.add(Dropout(0.20))\n",
    "#     model.add(LSTM(40, return_sequences=False))\n",
    "#     model.add(Dropout(0.20))\n",
    "#     model.add(Dense(output_dim, activation='softmax'))\n",
    "#\n",
    "#     model.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "#     history = model.fit(X_train, y_train, epochs=N_EPOCH, batch_size=BATCH_SIZE, verbose=2, shuffle=False)\n",
    "#\n",
    "#     return model, history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "39/39 - 4s - loss: 0.7010 - accuracy: 0.4990 - 4s/epoch - 115ms/step\n",
      "Epoch 2/100\n",
      "39/39 - 1s - loss: 0.6916 - accuracy: 0.5313 - 1s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "39/39 - 1s - loss: 0.6779 - accuracy: 0.5733 - 1s/epoch - 33ms/step\n",
      "Epoch 4/100\n",
      "39/39 - 1s - loss: 0.6744 - accuracy: 0.5948 - 1s/epoch - 33ms/step\n",
      "Epoch 5/100\n",
      "39/39 - 1s - loss: 0.6644 - accuracy: 0.6174 - 1s/epoch - 31ms/step\n",
      "Epoch 6/100\n",
      "39/39 - 1s - loss: 0.6602 - accuracy: 0.6257 - 1s/epoch - 32ms/step\n",
      "Epoch 7/100\n",
      "39/39 - 1s - loss: 0.6527 - accuracy: 0.6325 - 1s/epoch - 32ms/step\n",
      "Epoch 8/100\n",
      "39/39 - 1s - loss: 0.6484 - accuracy: 0.6349 - 1s/epoch - 33ms/step\n",
      "Epoch 9/100\n",
      "39/39 - 1s - loss: 0.6486 - accuracy: 0.6341 - 1s/epoch - 34ms/step\n",
      "Epoch 10/100\n",
      "39/39 - 1s - loss: 0.6456 - accuracy: 0.6397 - 1s/epoch - 32ms/step\n",
      "Epoch 11/100\n",
      "39/39 - 1s - loss: 0.6463 - accuracy: 0.6423 - 1s/epoch - 31ms/step\n",
      "Epoch 12/100\n",
      "39/39 - 1s - loss: 0.6411 - accuracy: 0.6425 - 1s/epoch - 32ms/step\n",
      "Epoch 13/100\n",
      "39/39 - 1s - loss: 0.6376 - accuracy: 0.6473 - 1s/epoch - 31ms/step\n",
      "Epoch 14/100\n",
      "39/39 - 1s - loss: 0.6401 - accuracy: 0.6457 - 1s/epoch - 31ms/step\n",
      "Epoch 15/100\n",
      "39/39 - 1s - loss: 0.6381 - accuracy: 0.6499 - 1s/epoch - 32ms/step\n",
      "Epoch 16/100\n",
      "39/39 - 1s - loss: 0.6372 - accuracy: 0.6531 - 1s/epoch - 36ms/step\n",
      "Epoch 17/100\n",
      "39/39 - 1s - loss: 0.6355 - accuracy: 0.6527 - 1s/epoch - 32ms/step\n",
      "Epoch 18/100\n",
      "39/39 - 1s - loss: 0.6330 - accuracy: 0.6545 - 1s/epoch - 31ms/step\n",
      "Epoch 19/100\n",
      "39/39 - 1s - loss: 0.6357 - accuracy: 0.6519 - 1s/epoch - 31ms/step\n",
      "Epoch 20/100\n",
      "39/39 - 1s - loss: 0.6314 - accuracy: 0.6541 - 1s/epoch - 32ms/step\n",
      "Epoch 21/100\n",
      "39/39 - 1s - loss: 0.6323 - accuracy: 0.6563 - 1s/epoch - 31ms/step\n",
      "Epoch 22/100\n",
      "39/39 - 1s - loss: 0.6296 - accuracy: 0.6581 - 1s/epoch - 32ms/step\n",
      "Epoch 23/100\n",
      "39/39 - 1s - loss: 0.6284 - accuracy: 0.6599 - 1s/epoch - 31ms/step\n",
      "Epoch 24/100\n",
      "39/39 - 1s - loss: 0.6289 - accuracy: 0.6575 - 1s/epoch - 32ms/step\n",
      "Epoch 25/100\n",
      "39/39 - 1s - loss: 0.6282 - accuracy: 0.6567 - 1s/epoch - 31ms/step\n",
      "Epoch 26/100\n",
      "39/39 - 1s - loss: 0.6273 - accuracy: 0.6619 - 1s/epoch - 31ms/step\n",
      "Epoch 27/100\n",
      "39/39 - 1s - loss: 0.6270 - accuracy: 0.6577 - 1s/epoch - 32ms/step\n",
      "Epoch 28/100\n",
      "39/39 - 1s - loss: 0.6249 - accuracy: 0.6637 - 1s/epoch - 31ms/step\n",
      "Epoch 29/100\n",
      "39/39 - 1s - loss: 0.6263 - accuracy: 0.6533 - 1s/epoch - 32ms/step\n",
      "Epoch 30/100\n",
      "39/39 - 1s - loss: 0.6241 - accuracy: 0.6601 - 1s/epoch - 31ms/step\n",
      "Epoch 31/100\n",
      "39/39 - 1s - loss: 0.6268 - accuracy: 0.6583 - 1s/epoch - 31ms/step\n",
      "Epoch 32/100\n",
      "39/39 - 1s - loss: 0.6244 - accuracy: 0.6607 - 1s/epoch - 31ms/step\n",
      "Epoch 33/100\n",
      "39/39 - 1s - loss: 0.6223 - accuracy: 0.6635 - 1s/epoch - 32ms/step\n",
      "Epoch 34/100\n",
      "39/39 - 1s - loss: 0.6233 - accuracy: 0.6579 - 1s/epoch - 32ms/step\n",
      "Epoch 35/100\n",
      "39/39 - 1s - loss: 0.6235 - accuracy: 0.6573 - 1s/epoch - 35ms/step\n",
      "Epoch 36/100\n",
      "39/39 - 1s - loss: 0.6203 - accuracy: 0.6641 - 1s/epoch - 36ms/step\n",
      "Epoch 37/100\n",
      "39/39 - 2s - loss: 0.6174 - accuracy: 0.6639 - 2s/epoch - 41ms/step\n",
      "Epoch 38/100\n",
      "39/39 - 1s - loss: 0.6181 - accuracy: 0.6719 - 1s/epoch - 37ms/step\n",
      "Epoch 39/100\n",
      "39/39 - 1s - loss: 0.6160 - accuracy: 0.6683 - 1s/epoch - 38ms/step\n",
      "Epoch 40/100\n",
      "39/39 - 1s - loss: 0.6154 - accuracy: 0.6681 - 1s/epoch - 36ms/step\n",
      "Epoch 41/100\n",
      "39/39 - 1s - loss: 0.6227 - accuracy: 0.6591 - 1s/epoch - 34ms/step\n",
      "Epoch 42/100\n",
      "39/39 - 2s - loss: 0.6186 - accuracy: 0.6635 - 2s/epoch - 45ms/step\n",
      "Epoch 43/100\n",
      "39/39 - 1s - loss: 0.6161 - accuracy: 0.6705 - 1s/epoch - 37ms/step\n",
      "Epoch 44/100\n",
      "39/39 - 1s - loss: 0.6079 - accuracy: 0.6747 - 1s/epoch - 36ms/step\n",
      "Epoch 45/100\n",
      "39/39 - 1s - loss: 0.6094 - accuracy: 0.6671 - 1s/epoch - 32ms/step\n",
      "Epoch 46/100\n",
      "39/39 - 1s - loss: 0.6073 - accuracy: 0.6752 - 1s/epoch - 32ms/step\n",
      "Epoch 47/100\n",
      "39/39 - 1s - loss: 0.6100 - accuracy: 0.6729 - 1s/epoch - 35ms/step\n",
      "Epoch 48/100\n",
      "39/39 - 1s - loss: 0.6089 - accuracy: 0.6758 - 1s/epoch - 33ms/step\n",
      "Epoch 49/100\n",
      "39/39 - 1s - loss: 0.6091 - accuracy: 0.6687 - 1s/epoch - 34ms/step\n",
      "Epoch 50/100\n",
      "39/39 - 1s - loss: 0.6097 - accuracy: 0.6711 - 1s/epoch - 33ms/step\n",
      "Epoch 51/100\n",
      "39/39 - 1s - loss: 0.6018 - accuracy: 0.6790 - 1s/epoch - 34ms/step\n",
      "Epoch 52/100\n",
      "39/39 - 1s - loss: 0.6004 - accuracy: 0.6820 - 1s/epoch - 31ms/step\n",
      "Epoch 53/100\n",
      "39/39 - 1s - loss: 0.5969 - accuracy: 0.6854 - 1s/epoch - 31ms/step\n",
      "Epoch 54/100\n",
      "39/39 - 1s - loss: 0.5976 - accuracy: 0.6860 - 1s/epoch - 31ms/step\n",
      "Epoch 55/100\n",
      "39/39 - 1s - loss: 0.5928 - accuracy: 0.6878 - 1s/epoch - 32ms/step\n",
      "Epoch 56/100\n",
      "39/39 - 1s - loss: 0.5958 - accuracy: 0.6858 - 1s/epoch - 32ms/step\n",
      "Epoch 57/100\n",
      "39/39 - 1s - loss: 0.5938 - accuracy: 0.6854 - 1s/epoch - 31ms/step\n",
      "Epoch 58/100\n",
      "39/39 - 1s - loss: 0.5881 - accuracy: 0.6916 - 1s/epoch - 33ms/step\n",
      "Epoch 59/100\n",
      "39/39 - 1s - loss: 0.5912 - accuracy: 0.6892 - 1s/epoch - 35ms/step\n",
      "Epoch 60/100\n",
      "39/39 - 1s - loss: 0.5865 - accuracy: 0.6908 - 1s/epoch - 31ms/step\n",
      "Epoch 61/100\n",
      "39/39 - 1s - loss: 0.5859 - accuracy: 0.6920 - 1s/epoch - 31ms/step\n",
      "Epoch 62/100\n",
      "39/39 - 1s - loss: 0.5920 - accuracy: 0.6872 - 1s/epoch - 32ms/step\n",
      "Epoch 63/100\n",
      "39/39 - 1s - loss: 0.5815 - accuracy: 0.6934 - 1s/epoch - 32ms/step\n",
      "Epoch 64/100\n",
      "39/39 - 1s - loss: 0.5790 - accuracy: 0.6938 - 1s/epoch - 32ms/step\n",
      "Epoch 65/100\n",
      "39/39 - 1s - loss: 0.5773 - accuracy: 0.6882 - 1s/epoch - 32ms/step\n",
      "Epoch 66/100\n",
      "39/39 - 1s - loss: 0.5763 - accuracy: 0.6954 - 1s/epoch - 33ms/step\n",
      "Epoch 67/100\n",
      "39/39 - 1s - loss: 0.5748 - accuracy: 0.6916 - 1s/epoch - 31ms/step\n",
      "Epoch 68/100\n",
      "39/39 - 1s - loss: 0.5738 - accuracy: 0.6972 - 1s/epoch - 31ms/step\n",
      "Epoch 69/100\n",
      "39/39 - 1s - loss: 0.5778 - accuracy: 0.6938 - 1s/epoch - 32ms/step\n",
      "Epoch 70/100\n",
      "39/39 - 1s - loss: 0.5675 - accuracy: 0.6994 - 1s/epoch - 31ms/step\n",
      "Epoch 71/100\n",
      "39/39 - 1s - loss: 0.5662 - accuracy: 0.7056 - 1s/epoch - 32ms/step\n",
      "Epoch 72/100\n",
      "39/39 - 1s - loss: 0.5648 - accuracy: 0.6992 - 1s/epoch - 31ms/step\n",
      "Epoch 73/100\n",
      "39/39 - 1s - loss: 0.5665 - accuracy: 0.7018 - 1s/epoch - 31ms/step\n",
      "Epoch 74/100\n",
      "39/39 - 1s - loss: 0.5651 - accuracy: 0.7062 - 1s/epoch - 31ms/step\n",
      "Epoch 75/100\n",
      "39/39 - 1s - loss: 0.5561 - accuracy: 0.7122 - 1s/epoch - 32ms/step\n",
      "Epoch 76/100\n",
      "39/39 - 1s - loss: 0.5609 - accuracy: 0.7048 - 1s/epoch - 31ms/step\n",
      "Epoch 77/100\n",
      "39/39 - 1s - loss: 0.5556 - accuracy: 0.7140 - 1s/epoch - 31ms/step\n",
      "Epoch 78/100\n",
      "39/39 - 1s - loss: 0.5550 - accuracy: 0.7020 - 1s/epoch - 31ms/step\n",
      "Epoch 79/100\n",
      "39/39 - 1s - loss: 0.5501 - accuracy: 0.7086 - 1s/epoch - 31ms/step\n",
      "Epoch 80/100\n",
      "39/39 - 1s - loss: 0.5496 - accuracy: 0.7096 - 1s/epoch - 31ms/step\n",
      "Epoch 81/100\n",
      "39/39 - 1s - loss: 0.5575 - accuracy: 0.7008 - 1s/epoch - 31ms/step\n",
      "Epoch 82/100\n",
      "39/39 - 1s - loss: 0.5468 - accuracy: 0.7100 - 1s/epoch - 31ms/step\n",
      "Epoch 83/100\n",
      "39/39 - 1s - loss: 0.5385 - accuracy: 0.7238 - 1s/epoch - 32ms/step\n",
      "Epoch 84/100\n",
      "39/39 - 1s - loss: 0.5438 - accuracy: 0.7150 - 1s/epoch - 31ms/step\n",
      "Epoch 85/100\n",
      "39/39 - 1s - loss: 0.5403 - accuracy: 0.7186 - 1s/epoch - 31ms/step\n",
      "Epoch 86/100\n",
      "39/39 - 1s - loss: 0.5432 - accuracy: 0.7104 - 1s/epoch - 31ms/step\n",
      "Epoch 87/100\n",
      "39/39 - 1s - loss: 0.5397 - accuracy: 0.7144 - 1s/epoch - 32ms/step\n",
      "Epoch 88/100\n",
      "39/39 - 1s - loss: 0.5404 - accuracy: 0.7178 - 1s/epoch - 32ms/step\n",
      "Epoch 89/100\n",
      "39/39 - 1s - loss: 0.5363 - accuracy: 0.7170 - 1s/epoch - 32ms/step\n",
      "Epoch 90/100\n",
      "39/39 - 1s - loss: 0.5227 - accuracy: 0.7267 - 1s/epoch - 32ms/step\n",
      "Epoch 91/100\n",
      "39/39 - 1s - loss: 0.5231 - accuracy: 0.7255 - 1s/epoch - 32ms/step\n",
      "Epoch 92/100\n",
      "39/39 - 1s - loss: 0.5233 - accuracy: 0.7303 - 1s/epoch - 31ms/step\n",
      "Epoch 93/100\n",
      "39/39 - 1s - loss: 0.5274 - accuracy: 0.7198 - 1s/epoch - 32ms/step\n",
      "Epoch 94/100\n",
      "39/39 - 1s - loss: 0.5200 - accuracy: 0.7333 - 1s/epoch - 33ms/step\n",
      "Epoch 95/100\n",
      "39/39 - 1s - loss: 0.5072 - accuracy: 0.7419 - 1s/epoch - 32ms/step\n",
      "Epoch 96/100\n",
      "39/39 - 1s - loss: 0.5104 - accuracy: 0.7339 - 1s/epoch - 33ms/step\n",
      "Epoch 97/100\n",
      "39/39 - 1s - loss: 0.5124 - accuracy: 0.7299 - 1s/epoch - 34ms/step\n",
      "Epoch 98/100\n",
      "39/39 - 1s - loss: 0.5001 - accuracy: 0.7347 - 1s/epoch - 36ms/step\n",
      "Epoch 99/100\n",
      "39/39 - 1s - loss: 0.5086 - accuracy: 0.7367 - 1s/epoch - 37ms/step\n",
      "Epoch 100/100\n",
      "39/39 - 1s - loss: 0.4990 - accuracy: 0.7477 - 1s/epoch - 32ms/step\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_8 (LSTM)               (None, 10, 63)            18396     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 10, 63)            0         \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 250)               314000    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 250)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 502       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 332,898\n",
      "Trainable params: 332,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model, history = basic_GRU(input_dim, output_dim, input_feature_size)\n",
    "model, history = basic_LSTM(input_dim, output_dim, input_feature_size)\n",
    "print(model.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 1s 9ms/step - loss: 0.8055 - accuracy: 0.6054\n",
      "Test loss: 0.805486798286438\n",
      "Test accuracy: 0.605423092842102\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "# classificatorName = 'Classificator{:.0f}.h5'.format(score[1]*100)\n",
    "# model.save('Models/'+classificatorName)\n",
    "model.save('ClassificationData/Models/Classificator30Days.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}